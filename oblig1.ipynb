{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "86cfdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import neighbors, datasets, model_selection, metrics, tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "cb8373b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Class to represent a node. Each node has a matrix X of data points and a list y of labels. \n",
    "Each node can also have a left child, a right child, a split_index, split_average and a \n",
    "most common label.\n",
    "\"\"\"\n",
    "class Node:\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        left_child = None,\n",
    "        right_child = None,\n",
    "        split_index = None,\n",
    "        split_average = None,\n",
    "        mcl = None\n",
    "    ):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.split_index = split_index\n",
    "        self.split_average = split_average\n",
    "        self.mcl = mcl\n",
    "    \n",
    "    #Function to check if the given node is a leaf node.\n",
    "    def is_leaf(self):\n",
    "        return self.left_child == None and self.right_child == None\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Class to represent a decision tree classifier. Each tree has a root node. \n",
    "\"\"\"\n",
    "class Tree:\n",
    "    def __init__(\n",
    "            self,\n",
    "            root = None\n",
    "        ):\n",
    "            self.root = root\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Funtion to calculate gini value of a node, based on the node's y list of labels. \n",
    "    \"\"\"\n",
    "    def gini(self, y):\n",
    "        \n",
    "        #get the different values of y (h and g), and how many occurances these have\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        gini = 1\n",
    "        \n",
    "        for i in range(len(values)):\n",
    "            prob = counts[i]/len(y)\n",
    "            gini -= pow(prob,2)\n",
    "        return gini\n",
    "     \n",
    "        \n",
    "    \"\"\"    \n",
    "    Funtion to calculate entropy value of a node, based on the node's y list of labels. \n",
    "    \"\"\"\n",
    "    def entropy(self, y):\n",
    "\n",
    "        #get the different values of y (h and g), and how many occurances these have\n",
    "        counts = np.unique(y, return_counts=True)\n",
    "        entropy = 0\n",
    "\n",
    "        for i in range(len(counts[0])):\n",
    "            prob = counts[1][i]/len(y)\n",
    "            entropy += prob*np.log2(prob)\n",
    "        return (-entropy)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate information gain from a split, based on the given impurity measure.\n",
    "    \"\"\"\n",
    "    def information_gain(self, parent_y, left_y, right_y, impurity_measure):\n",
    "        \n",
    "        weight_left = len(left_y)/len(parent_y)\n",
    "        weight_right = len(right_y)/len(parent_y)\n",
    "        \n",
    "        information = 0\n",
    "        impurity_parent = 0\n",
    "        \n",
    "        #calculate the information based on the impurity measure\n",
    "        if impurity_measure == \"gini\":\n",
    "            gini_left = self.gini(left_y)\n",
    "            gini_right = self.gini(right_y)\n",
    "            impurity_parent = self.gini(parent_y)\n",
    "            information = weight_left*gini_left + weight_right*gini_right   \n",
    "        else:\n",
    "            en_left = self.entropy(left_y)\n",
    "            en_right = self.entropy(right_y)\n",
    "            impurity_parent = self.entropy(parent_y)\n",
    "            information = weight_left*en_left + weight_right*en_right\n",
    "        \n",
    "        gain = impurity_parent - information\n",
    "        return gain\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to check if a node has identical features.\n",
    "    \"\"\"\n",
    "    def has_identical_features(self, X):\n",
    "        \n",
    "        for i in range(len(X[0])):\n",
    "            #make a list of all values in the current column\n",
    "            features = [col[i] for col in X]\n",
    "            \n",
    "            #if all the values in this column are equal, return True\n",
    "            if np.all(features == features[0]):\n",
    "                return True, features\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Funtion to find a node's most common label.\n",
    "    \"\"\"\n",
    "    def most_common_label(self, node):\n",
    "        \n",
    "        #if the given node is a leaf, all y-values are the same\n",
    "        if node.is_leaf():\n",
    "            return node.y[0]\n",
    "    \n",
    "        else:\n",
    "            values, counts = np.unique(node.y, return_counts = True)\n",
    "            \n",
    "            if counts[0] > counts[1]:\n",
    "                return values[0]\n",
    "            else:\n",
    "                return values[1]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to divide given data based on the split index and average at each node.\n",
    "    \"\"\"\n",
    "    def divide_data(self, node, X, y):\n",
    "        \n",
    "        #make a list of all values in the split-index-column\n",
    "        features = [col[node.split_index] for col in X]\n",
    "        \n",
    "        left_y = [] \n",
    "        left_X = []\n",
    "        right_y = []\n",
    "        right_X =[]\n",
    "        \n",
    "        #loop through all the values in the column\n",
    "        for i in range(len(features)):\n",
    "            \n",
    "            #split the data based on the split average\n",
    "            if features[i] < node.split_average:\n",
    "                left_X.append(X[i])\n",
    "                left_y.append(y[i])\n",
    "                \n",
    "            else:\n",
    "                right_X.append(X[i])\n",
    "                right_y.append(y[i])\n",
    "                \n",
    "        return left_X, left_y, right_X, right_y\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to prune a tree, starting from the root of the tree, based on given prune data.\n",
    "    \"\"\"\n",
    "    def prune(self, tree_root, prune_X, prune_y):\n",
    "\n",
    "        #if the root of the current split is a leaf, return the accuracy of this node\n",
    "        if tree_root.is_leaf():\n",
    "            return prune_y.count(tree_root.mcl) #self.calc_error(tree_root.mcl, prune_y)\n",
    "        \n",
    "        #use divide_data to divide the prune data\n",
    "        left_X, left_y, right_X, right_y = self.divide_data(tree_root, prune_X, prune_y)\n",
    "        \n",
    "        #get the accuracy of the prune data in the child nodes with recursion\n",
    "        left_acc = self.prune(tree_root.left_child, left_X, left_y)\n",
    "        right_acc = self.prune(tree_root.right_child, right_X, right_y)\n",
    "        \n",
    "        parent_acc = prune_y.count(tree_root.mcl) #self.calc_error(tree_root.mcl, prune_y)\n",
    "        \n",
    "        #if none of the prune data went to one of the child nodes, pruning won't occur\n",
    "        if left_acc != None and right_acc != None:\n",
    "            \n",
    "            #if parent accuracy is better than the child nodes', prune this sub tree.\n",
    "            if parent_acc > left_acc + right_acc:\n",
    "                \n",
    "                tree_root.left_child = None\n",
    "                tree_root.right_child = None\n",
    "                return parent_acc\n",
    "            \n",
    "        return left_acc + right_acc\n",
    "                                     \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to split the given node into two child nodes, left and right, \n",
    "    based on the impurity measure\n",
    "    \"\"\"\n",
    "    def split(self, node, impurity_measure):\n",
    "\n",
    "        #get the number of feature rows and columns\n",
    "        num_f_rows, num_f_cols = np.shape(node.X)\n",
    "        \n",
    "        opt_gain = 0\n",
    "        \n",
    "        for i in range(num_f_cols):\n",
    "            \n",
    "            #make a list of all values in the current column\n",
    "            features = [col[i] for col in node.X]\n",
    "            \n",
    "            #calculate the value to do the splitting after\n",
    "            feature_average = np.average(features)\n",
    "                \n",
    "            left = Node([],[])\n",
    "            right = Node([],[])\n",
    "        \n",
    "            #loop through each value in the column, and assign them to a child node, based on the feature average\n",
    "            for j in range(num_f_rows):\n",
    "                if features[j] < feature_average:\n",
    "                    left.X.append(node.X[j])\n",
    "                    left.y.append(node.y[j])\n",
    "                    \n",
    "                else:\n",
    "                    right.X.append(node.X[j])\n",
    "                    right.y.append(node.y[j])\n",
    "                    \n",
    "            #information gain for the current split       \n",
    "            gain = self.information_gain(node.y, left.y, right.y, impurity_measure)\n",
    "             \n",
    "            #check if current split is the optimal one    \n",
    "            if (gain > opt_gain):\n",
    "                opt_gain = gain\n",
    "                node.left_child = left\n",
    "                node.right_child = right\n",
    "                node.split_index = i\n",
    "                node.split_average = feature_average\n",
    "            \n",
    "        return node.left_child, node.right_child    \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to grow the tree from a given root node\n",
    "    \"\"\"\n",
    "    def grow_tree(self, node, impurity_measure):\n",
    "        \n",
    "        #assign the current node it's most common label\n",
    "        node.mcl = self.most_common_label(node)\n",
    "        \n",
    "        #if the entropy of a node is 0, it's y values are pure\n",
    "        if self.entropy(node.y) == 0:\n",
    "            node.mcl = node.y[0]\n",
    "            return \n",
    "\n",
    "        elif self.has_identical_features(node.X):\n",
    "            node.mcl = most_common_label(node)\n",
    "            return \n",
    "\n",
    "        else: \n",
    "            #split the node\n",
    "            left, right = self.split(node, impurity_measure)\n",
    "            \n",
    "            #continue growing the tree\n",
    "            self.grow_tree(left, impurity_measure)\n",
    "            self.grow_tree(right, impurity_measure)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to create a decision tree based on given X and y data, impurity_measure and pruning\n",
    "    \"\"\"\n",
    "    def learn(self, X, y, impurity_measure=\"entropy\", pruning=False):\n",
    "\n",
    "        seed = 300\n",
    "        \n",
    "        #splitting the training data into training and pruining\n",
    "        X_train, X_prune, y_train, y_prune = model_selection.train_test_split(\n",
    "        X, y, test_size = 0.3, random_state = seed)\n",
    "        \n",
    "        #creating the root of the tree\n",
    "        self.root = Node(X_train, y_train)\n",
    "        \n",
    "        #grow the tree\n",
    "        self.grow_tree(self.root, impurity_measure)\n",
    "        \n",
    "        #prune tree if pruning is true\n",
    "        if pruning:\n",
    "            self.prune(self.root, X_prune, y_prune)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate total accuracy of the tree, based on given data\n",
    "    \"\"\"\n",
    "    def calc_total_accuracy(self, X, y):\n",
    "        pred = []\n",
    "        \n",
    "        for x in X:\n",
    "            pred.append(self.predict(x, self.root))\n",
    "         \n",
    "        return metrics.accuracy_score(y, pred)\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to predict the label of a given x data point\n",
    "    \"\"\"\n",
    "    def predict(self, x, node):\n",
    "\n",
    "        if node.is_leaf():\n",
    "            return node.mcl\n",
    "        \n",
    "        if (x[node.split_index] < node.split_average):\n",
    "            return self.predict(x, node.left_child)\n",
    "        else:\n",
    "            return self.predict(x, node.right_child)\n",
    "                                                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "d0beb37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to my decision tree! :)\n",
      "_______________________________\n",
      "\n",
      "First, we need to grow the tree using our training dataset:\n",
      "Growing tree ...\n",
      "\n",
      "Tree is fully grown!\n",
      "\n",
      "Next, we should prune the tree a little bit. It is way too big!\n",
      "Pruning tree ...\n",
      "\n",
      "Tree is pruned!\n",
      "_______________________________\n",
      "\n",
      "Now, we want to check if our tree is any good for a different set of data. We will call this out validation dataset.\n",
      "\n",
      "Accuracy on validation dataset with ...\n",
      "- entropy and without pruning: 79.21486154924641%\n",
      "- gini and without pruning: 78.7241500175254%\n",
      "- entropy and with pruning: 83.56116368734665%\n",
      "- gini and with pruning: 83.59621451104101%\n",
      "\n",
      "The accuracy with the validation dataset is slighly different when using the different settings, so let's choose the best one.\n",
      "\n",
      "Gini with pruning was the best setting!\n",
      "\n",
      "Now, lets test our implementation with the test dataset on our chosen model.\n",
      "\n",
      "Accuracy on test dataset with the best model: 83.87662110059586%\n",
      "\n",
      "Pretty good! :)\n",
      "_______________________________\n",
      "\n",
      "Growing a decision tree with sklearn's Decision Tree Classifier ...\n",
      "Done! That was fast!\n",
      "Accuracy score with gini: 82.19418156326674%\n",
      "Accuracy score with entropy: 82.40448650543287%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function to select the best model, based on the accuracies given\n",
    "\"\"\"\n",
    "def select_best_model(val_accuracies):\n",
    "    max_model = max(val_accuracies, key=val_accuracies.get)\n",
    "    max_acc = val_accuracies.get(max_model)\n",
    "    return max_model\n",
    "\n",
    "data = pd.read_csv(\"magic04.data\", header=None)\n",
    "\n",
    "X = data.values[:,:-1].tolist()\n",
    "y = data.values[:,-1].tolist()\n",
    "\n",
    "seed = 300\n",
    "\n",
    "#splitting the dataset into training and test/validation\n",
    "X_train, X_val_test, y_train, y_val_test = model_selection.train_test_split(\n",
    "X, y, test_size = 0.3, random_state=seed)\n",
    "        \n",
    "#splitting the validation/testing data \n",
    "X_val, X_test, y_val, y_test = model_selection.train_test_split(\n",
    "X_val_test, y_val_test, test_size = 0.5, random_state = seed)\n",
    "\n",
    "\n",
    "print(\"Welcome to my decision tree! :)\\n_______________________________\\n\\nFirst, we need to grow the tree using our training dataset:\")\n",
    "print(\"Growing tree ...\\n\")\n",
    "\n",
    "tree_en = Tree().learn(X_train, y_train)\n",
    "tree_gi = Tree().learn(X_train, y_train, \"gini\")\n",
    "\n",
    "print(\"Tree is fully grown!\\n\")\n",
    "\n",
    "print(\"Next, we should prune the tree a little bit. It is way too big!\\nPruning tree ...\\n\")\n",
    "\n",
    "tree_en_pr = Tree().learn(X_train, y_train, \"entropy\", True)\n",
    "tree_gi_pr = Tree().learn(X_train, y_train, \"gini\", True)\n",
    "\n",
    "print(\"Tree is pruned!\\n_______________________________\\n\")\n",
    "print(\"Now, we want to check if our tree is any good for a different set of data. We will call this out validation dataset.\\n\")\n",
    "print(\"Accuracy on validation dataset with ...\")\n",
    "\n",
    "val_acc_en = tree_en.calc_total_accuracy(X_val, y_val)\n",
    "val_acc_gi = tree_gi.calc_total_accuracy(X_val, y_val)\n",
    "val_acc_en_pr = tree_en_pr.calc_total_accuracy(X_val, y_val)\n",
    "val_acc_gi_pr = tree_gi_pr.calc_total_accuracy(X_val, y_val)\n",
    "\n",
    "print(f\"- entropy and without pruning: {val_acc_en*100}%\")\n",
    "print(f\"- gini and without pruning: {val_acc_gi*100}%\")#train_acc_gi_val\n",
    "print(f\"- entropy and with pruning: {val_acc_en_pr*100}%\")#train_acc_en_pr_val\n",
    "print(f\"- gini and with pruning: {val_acc_gi_pr*100}%\")#train_acc_gi_pr_val\n",
    "\n",
    "print(\"\\nThe accuracy with the validation dataset is slighly different when using the different settings, so let's choose the best one.\\n\")\n",
    "\n",
    "val_accuracies = {\n",
    "    \"Entropy without pruning\" : train_acc_en_val,\n",
    "    \"Gini without pruning\" : train_acc_gi_val,\n",
    "    \"Entropy with pruning\" : train_acc_en_pr_val,\n",
    "    \"Gini with pruning\" : train_acc_gi_pr_val\n",
    "}\n",
    "\n",
    "best_model = select_best_model(val_accuracies)\n",
    "\n",
    "print(f\"{best_model} was the best setting!\\n\")\n",
    "print(\"Now, lets test our implementation with the test dataset on our chosen model.\\n\")\n",
    "\n",
    "test_acc = tree_gi_pr.calc_total_accuracy(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy on test dataset with the best model: {test_acc*100}%\\n\")\n",
    "print(\"Pretty good! :)\")\n",
    "print(\"_______________________________\\n\")\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ecfb50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
